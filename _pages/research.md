---
layout: archive
title: "Research"
permalink: /research/
author_profile: true
---

## Overview

My research is **interdisciplinary**, spanning high-performance computing, graph and sparse algorithms, and large-scale machine learning, with applications across scientific computing, data science, and AI systems.

At a high level, my group focuses on **designing scalable algorithms and systems** that make emerging data- and compute-intensive workloads practical on modern supercomputers. This work integrates ideas from numerical linear algebra, graph theory, distributed systems, and machine learning, and is motivated by real-world applications in science and engineering.

Here are some **key research thrusts** pursued in our group.
For a **complete and up-to-date list of publications**, please see my  
ðŸ‘‰ **Google Scholar profile**.

---

## Research Themes

---

## 1. Sparse Linear Algebra at Scale

::contentReference[oaicite:0]{index=0}


### Focus
Developing high-performance sparse matrix kernels and algorithms for CPUs, GPUs, and distributed-memory systems.

### Sub-projects
- Scalable SpMM and SpGEMM algorithms
- Communication-avoiding sparse computations
- GPU acceleration for irregular sparse workloads
- Performance modeling and benchmarking

### Representative Papers
- *Filler Paper Title on Scalable SpGEMM*, Conference YYYY  
- *Filler Paper Title on GPU Sparse Kernels*, Journal YYYY  
- *Filler Paper Title on Communication-Aware Sparse Algebra*, Conference YYYY

---

## 2. Graph Algorithms and Network Analysis

::contentReference[oaicite:1]{index=1}


### Focus
Designing scalable graph algorithms for clustering, partitioning, and analysis of massive networks.

### Sub-projects
- Distributed graph clustering and community detection
- Scalable graph partitioning
- Embedding and representation learning for graphs
- Performance-aware graph analytics

### Representative Papers
- *Filler Paper Title on Distributed Graph Clustering*, Conference YYYY  
- *Filler Paper Title on Large-Scale Network Analysis*, Journal YYYY

---

## 3. Distributed and Scalable Machine Learning Systems

::contentReference[oaicite:2]{index=2}


### Focus
Building communication-efficient systems for training and inference of large-scale ML models on supercomputers.

### Sub-projects
- Data, model, and pipeline parallel training
- Communication optimization for ML workloads
- Sparse and structured ML computations
- System-level performance analysis

### Representative Papers
- *Filler Paper Title on Distributed ML Training*, Conference YYYY  
- *Filler Paper Title on Communication-Efficient ML*, Journal YYYY

---

## 4. Interpretable and Explainable Graph ML

::contentReference[oaicite:3]{index=3}


### Focus
Developing scalable and interpretable methods for understanding predictions made by graph neural networks and related models.

### Sub-projects
- Distributed explainability algorithms
- Shapley-value-based explanations for graphs
- Scalable GNN interpretation pipelines
- Trade-offs between accuracy, cost, and interpretability

### Representative Papers
- *Filler Paper Title on GNN Explainability*, Conference YYYY  
- *Filler Paper Title on Scalable Model Interpretation*, Journal YYYY

---

## 5. HPC for Scientific and Data-Driven Applications

::contentReference[oaicite:4]{index=4}


### Focus
Applying HPC and scalable algorithms to real-world scientific and engineering problems.

### Sub-projects
- Data-driven scientific modeling
- Large-scale simulation and analytics pipelines
- Integration of ML with traditional HPC workflows
- Performance and energy efficiency studies

### Representative Papers
- *Filler Paper Title on HPC Scientific Applications*, Journal YYYY  
- *Filler Paper Title on Data-Driven Science*, Conference YYYY

---

## 6. Software and Reproducible Research Infrastructure

::contentReference[oaicite:5]{index=5}


### Focus
Building open-source, reusable, and reproducible software frameworks for HPC and ML research.

### Sub-projects
- Open-source HPC libraries
- Reproducible benchmarking pipelines
- Research software engineering for performance portability
- Education-focused HPC software tools

### Representative Papers
- *Filler Paper Title on Reproducible HPC Software*, Journal YYYY

---

## Publications

For the **full and most recent publication list**, including preprints and citations, please refer to:

ðŸ‘‰ **Google Scholar**

(Selected publications are highlighted above under each research theme.)

---

## Future Project Pages

Several of the research thrusts listed above include **large, multi-year projects**.  
Dedicated pages for these projects will be added in the future and linked internally, without cluttering the top-level navigation.

